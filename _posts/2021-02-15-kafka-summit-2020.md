---
layout: post
authors: [tom_van_den_bulck]
title: 'Kafka Summit 2020'
image: /img/2020-10-05-kafka-summit/thumb-resized.png
tags: [Kafka, Conference]
category: Cloud
comments: true
---

# Table of contents
{:.no_toc}

- TOC
{:toc}

----

# Introduction

Attending conferences in these times tends to be a pretty new experience, with everything being virtual.

The Kafka Summit 2020 in London was cancelled, but the one in Austin was not.
However, it was transformed, transformed in a new format, completely COVID proof, fully remote and virtual.

I love conferences, as these bring me knew knowledge and insights.
But, my experience with virtual ones is quite limited.

Please note, working remote is no issue, as I have become a true senior remote developer in these last lockdowns.

# Day 1

## Where is Day 1?
Timezone difference combined with a busy day at work is not that ideal.

<<understatement>>
    
Since the coffee was no longer helping I decided to dive in the bed in order to charge up the batteries for the next day.

# Day 2

## Keynote The Tyranny of Data by Sam Newman https://twitter.com/samnewman
In this keynote Sam Newman gave a talk about the tyranny of data and how you can harnass this, you can rewatch the talk [here](https://www.confluent.io/resources/kafka-summit-2020/sam-newman-building-microservices-keynote-the-tyranny-of-data){:target="_blank" rel="noopener noreferrer"}.

Data is the lifeblood of what we do, 75% of your time is taking data out of a database and putting it on a screen.

A relational DB is a wonderful thing, it has transactions, it is robust.
But it wants to have all the data and they tend to become too big to fail at which point it becomes that important that we are afraid to change it.

Breaking apart data can be interesting to reduce the scope of fulfilling regulations.
Also distributing responsibility in order to avoid the need to constantly coördinate between teams.

This leads us to microservices which give us the property of independent deployability: when implemented correctly, it can really help isolate the surface area of failure.

It is important to get the boundaries of the services right, together with independent deployability it can be possible.

Backwards compatibility is important to maintain, there is an idea from 1971: [Information hiding](https://en.wikipedia.org/wiki/Information_hiding){:target="_blank" rel="noopener noreferrer"}.
The idea is to hide as much information as you can, because as soon as you expose it, it becomes part of the contract with the outside world.

Hiding data is vital, without this no backwards compatibility as you will be unable to change something without breaking this for one of your consumers.

Another advantage of splitting apart data allows us to do more work in parallel

However there are some pain points, you miss out where data is no longer together.

We all know the feature request where business people want to put one more field on the screen, because this is so conventient for the user.
Completely oblivious to the boundaries which have been defined within system of microservices.

Kafka can help you with that as it is a nice central backbone, which can be used as a shared place where data can be interchanged.

As a backbone the simplest way to use kafka is with an event driven architecture.

The permancence properties can make new consumers pick up older events.
This allows us to shift our thought processes to make events more permanent and resolve some of the issues of splitting out this data.

Isn’t sharing events not like sharing a database?
An event is much smaller in scope, easier to reason about and easier to keep it compatible.

When comparing with a database the questions around joins and ad-hoc queries always arise.

Various options exist for these, one of these is to store the events in another database: [Reporting database pattern](https://martinfowler.com/bliki/ReportingDatabase.html){:target="_blank" rel="noopener noreferrer"}.
As this decouples data, about how the application uses its own data and how other tools can view this data.

A very usefull tool which you can use for this is [Debezium](https://debezium.io/){:target="_blank" rel="noopener noreferrer"}.
It changes the data capture process as it views to the commit log and it is also very useful for hooking into legacy systems.
Much easier the missing with legacy code.

Another approach is when you start crossing the various events streams within kafka.
<div style="text-align: center;">
  <img alt="Crossing the streams" src="/img/2020-10-05-kafka-summit/cross-streams.png" width="auto" height="auto" target="_blank" class="image fit">
</div>

The effect will not be like Ghostbusters, but it will enable you to combine your events streams in realtime.

A nice feature to cross these streams is a [Push query](https://docs.ksqldb.io/en/latest/concepts/queries/push/){:target="_blank" rel="noopener noreferrer"} where a client can subscribe to results of that query and consume events off that stream as these happen in realtime.
<div style="text-align: center;">
  <img alt="Crossing the streams" src="/img/2020-10-05-kafka-summit/push-query.png" width="auto" height="auto" target="_blank" class="image fit">
</div>

Some book suggestions by Sam are: 
* [Designing Event-Driven Systems](https://www.confluent.io/designing-event-driven-systems/){:target="_blank" rel="noopener noreferrer"}: Good read and pretty short, always nice.

* [Monolith to microservices](https://samnewman.io/books/monolith-to-microservices/){:target="_blank" rel="noopener noreferrer"}: Which describes how you can break up your data, among many other things.


## Future of Kafka by [Gwen Shapira](https://twitter.com/gwenshap){:target="_blank" rel="noopener noreferrer"}
The next session was a more dynamic session, not recorded, and also with a limited amount of people who could join.

There was a chat window, which is nice because you are dealing with a limited set of people, but when trying to read that you lost focus of the screen and the speaker, which is kinda annoying.
But still a nice thought and I bet will be improved for upcoming virtual sessions.

### The previous day
{:.no_toc}
It started with some basic questions, one of these was about what other people found interesting talks of the previous day.
So because I wasn't there and have no idea which might be interesting, I have also recorded those answers
* Activision: going from batch to streaming, in their talk [Bravo Six, Going Realtime. Transitioning Activision Data Pipeline to Streaming](https://www.confluent.io/resources/kafka-summit-2020/bravo-six-going-realtime-transitioning-activision-data-pipeline-to-streaming/){:target="_blank" rel="noopener noreferrer"}
* Talk about bloomberg about a stretch cluster: [Fully-Managed, Mutli-Tenant Kafka Clusters: Tips, Tricks and Tools](https://www.confluent.io/resources/kafka-summit-2020/fully-managed-multi-tenant-kafka-clusters-tips-tricks-and-tools/){:target="_blank" rel="noopener noreferrer"}
* [KafkaConsumer - Decoupling Consumption and Processing for Better Resource Utilization](https://www.confluent.io/resources/kafka-summit-2020/kafkaconsumer-decoupling-consumption-and-processing-for-better-resource-utilization/){:target="_blank" rel="noopener noreferrer"}, had some interesting points
* And the final recommendation [Kafka Lag Monitoring For Human Beings](https://www.confluent.io/resources/kafka-summit-2020/kafka-lag-monitoring-for-human-beings/){:target="_blank" rel="noopener noreferrer"}

### Stretching your sanity
{:.no_toc}
Working with 2 cloud providers apparently stretches your sanity, the APIs are similar but behaviour is different. 
Like for example disk usage: some providers allow you to expand the disk when the machine is running while one doesn’t.
All those things make it so that a lot of your automation goes out of the window, or that it becomes a forest of if statements.

### Q & A
{:.no_toc}
#### I was intrigued by your wish list for the future of Kafka. I was hoping to hear some of those ideas you were afraid to say during the keynote.
{:.no_toc}

Debugability and observabilty are key requirements by Gwen for her team (kafka as a service on cloud providers

Concerns about the network layer - lot of stuff was made when Netty was still new (discovery burden is part of the protocol) does not play well with Kubernetes

And other efficiency and scalability improvements.

#### As GDPR is a big topic in Europe, any approaches efforts to support crypto shredding natively?
{:.no_toc}

Currently no encryption at rest and other stuff - There is lots of room to make encryption a 1st lvl concern for Kafka

#### You just mentioned a new edition of the definitive guide, what will be the new additions? Also thx for it, it helps a lot to understand the internals.
{:.no_toc}

Big changes around administration, new metrics to monitor and new best practices.
And a chapter around security.

Early publish for next week

#### I heard a lot about if exactly once semantics but never saw a clear example or in house solution for consumer side. Any improvement on Consumer API related to that. Thanks , you're an amazing speaker!
{:.no_toc}

On the consumer side: configuration - either get committed messages as a consumer or you get all.
If you only get committed messages it will increase the latency.
Producer should write the offsets (if you are a consumer - producer)

#### Hi My key wishlist of Kafka is that a put on a Kafka topic can participate in an XA two phase commit
{:.no_toc}

This type of integration should be part of the connect framework


#### Are there any tools that you know of to help manage/document/understand the web of topics, producers, consumers, and streams? I'm thinking about Operations staff effectively managing and multi-tenant environment, or Architects discovering and domain ownership and modeling new change.
{:.no_toc}

[Strimzi](https://strimzi.io/){:target="_blank" rel="noopener noreferrer"}
Which can integrate nicely with github where you can have yamls describing topics.

#### Any intended future support for postponed messages (scheduled messages) ?
{:.no_toc}

No, the main challenge is if this can be implemented performantly.

#### I'm curious whether there are anything planned to support more topic/partitions in a Kafka cluster (besides KIP-500). In particular I found that the Java producer client has some bottlenecks when publishing to a wide range of topics, (due to usage of CopyOnWriteMap and GC pressure). Also wondering if there folks out there trying push partition limit boundaries and can share some tips :)
{:.no_toc}

Fantastic question, lots of time was spent on improving performance of the brokers / producers

Partition limit is important to consider as with big clusters there can be a lot of replication overhead.

#### How efficient of the native compression of Kafka? Should we compress the content by ourself?
{:.no_toc}

No, don’t compress data yourselves.
Kafka can send messages via batches, see delay configuration.
Kafka compresses these batches as these batches tend to contain lots of related data so this results in a better compression result

There are 4 algorithms to choose from: none, gzip, lz4 and snappy

#### We are having issues with configuring load balancer in front of the RestProxies for consumers with certificate based authentication. anything new comming up in improving in this area? thank you.
{:.no_toc}

There is currently lots of tooling improvements in progress.

## Learnings from the Field. Lessons from Working with Dozens of Small & Large Deployments

In this session lots of hints and tips of using Kafka were addressed.

One of the most important is to make sure to upgrade often, as new releases always contain lots of bug fixes and other small improvements.

Another very important topic to consider are quotas.

* One of the most important ones is the leader replication throttle rate 'leader.replication.throttled.rate'.
Because by default a rebalance is unthrottled, which means that it can use up all the bandwith of your cluster.

* Also there are client quotas to consider: bandwidth and client quotas.

<div style="text-align: center;">
  <img alt="Crossing the streams" src="/img/2020-10-05-kafka-summit/pitfall.png" target="_blank" class="image fit" style="magin:0px auto; max-width:400px">
</div>

An interesting session, be sure to [check it out](https://www.confluent.io/resources/kafka-summit-2020/learnings-from-the-field-lessons-from-working-with-dozens-of-small-large-deployments/){:target="_blank" rel="noopener noreferrer"} if you are planning on running kafka, or to make sure that you do not fall in the same pittfalls as countless others have done before you.


## Exactly-Once Made Easy: Transactional Messaging Improvement for Usability & Scalability
By Boyang Chen and [Guozhang Wang](https://twitter.com/guozhangwang){:target="_blank" rel="noopener noreferrer"}

Their talk can be found [here](https://www.confluent.io/resources/kafka-summit-2020/exactly-once-made-easy-transactional-messaging-improvement-for-usability-and-scalability){:target="_blank" rel="noopener noreferrer"}.

If you want to learn more about how exactly-once was implemented in kafka you should watch this talk.
It describes how kafka transactions work and how these came to be.

An important note is that it will only work if your app is only consuming and producing to kafka.
Transactions with other (external) systems, like a database, can not be covered by this solution.

Only 1 transaction per partition is possible, this will also have an impact on performance.

## Experienced Developers
This session was one of those discussion ones and it was full.

I am not sure what the limit is of these rooms, but that is kinda annoying.
It would be nice if they could show a counter on the schedule about how full that session is, it is all virtual so that should be possible


## Maximize Business Value of Machine Learning and Data Science with Kafka
By Chirag Dadia & Tom Szumowski, Nuuly

Nuuly is a company which rents out clothing via a monhtly subscription plan.

Kafka streams is used pretty heavily, all micro-services have stream processors and these processors tend to handle most of the business logic.

Kafka is used as the persistent data store, this actually allows them to use their Kafka subsystem and services as a raw data lake which feeds into the downstream applications.
Most workflows are asychronously processed.

Kafka is the backbone at Nuuly, it holds all the data, it is the business logic processor and it is orchestrating all the workflows.
Adding a new step into a workflow is as simple as adding a new processor / microservice and the needed topics, easy.

Some use cases where Kafka is used within Nuuly:
* Personalization: recommendation.
* Order fullfillment: minimize the walking path in the warehouse.
* Online optimization: real-time feedback streams for user feedback.

For each of these use cases they go deeper into how these were implemented in the [talk](https://www.confluent.io/resources/kafka-summit-2020/maximize-the-business-value-of-machine-learning-and-data-science-with-kafka){:target="_blank" rel="noopener noreferrer"}.

KSQL is currently being investigated as it is now a managed service, but not yet actively used.

# Final Thoughts
Kafka Summit 2020 was a nicely handled virtual conference, one thing which I did not like was the lack of coffee breaks in between the sessions. 

It was possible to reach out to a tech support to address possible issues.

There was also a nice attempt to bring some concept of booths into the conference by allowing sponsors to setup virtual booths.

An simulating people to attend these booths by providing badges, badges are always cool.

But for me the main issue is that in a virtual conference you mis out on a lot of extra discussions which happens between the participants.
For me these discussions are an importan part of a conference as often these side-discussions provide you with much more insight and knowledge then most of the talks on schedule.

On the other hand, if you just prefer to watch talks then a virtual conference will be much more pleasant and hassle free and enabling you to attend a conference which would otherwhise be impossible or too expensive to attend to.
So in that aspect it might also enable us to spread more knowledge and not only to the lucky few which are able to travel.

However for a virtual conference you will still need to be well rested in order to make the most of it, so combining it with a work day is far from ideal.

And plan for the timezone difference.
 
