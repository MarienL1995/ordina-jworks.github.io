---
layout: post
authors: [tom_van_den_bulck]
title: 'Kafka Summit 2020'
image: /img/2020-09-25-ghelamco-alert/rpi-front.jpg
tags: [Kafka, Conference]
category: Cloud
comments: true
---

# Table of contents
{:.no_toc}

- TOC
{:toc}

----

# Introduction

Attending conferences in these times tends to be a pretty new experience, with everything being virtual.

The Kafka Summit 2020 in London was cancelled, but the one in Austin was not.
However, it was transformed.
Transformed in a new format, completely COVID proof, fully remote and virtual.

I love conferences, as these bring me knew knowledge and insights.

But, my experience with virtual ones is quite limited.

Please note, working remote is no issue, as I have become a true senior remote developer in the last 6 months.


# Day 2

## Where is Day 1?
Timezone difference combined with a busy day at work is not that ideal.

<<understatement>>
    
Since the coffee was no longer helping I decided to dive in the bed in order to charge up the batteries for the next day.

## Keynote The Tyranny of Data by Sam Newman

https://www.confluent.io/resources/kafka-summit-2020/sam-newman-building-microservices-keynote-the-tyranny-of-data


Data is the lifeblood of what we do, 75% of your time is taking data out of a database and putting it on a screen.

A relational DB is a wonderful thing, it has transactions, it is robust.
But it wants to have all the data and they tend to become too big to fail at which point it becomes that important that we are afraid to change it.

Breaking apart data can be interesting to reduce the scope of fulfilling regulations.

Property of independent deployability: that is what we want from a microservice architecture.
Distributing responsibility avoid the need to constantly coördinate between teams.

It is important to get the boundaries of the services right, together with independent deployability it can be possible.

Backwards compatibility is important to maintain.
There is an idea from 1971: [Information hiding](https://en.wikipedia.org/wiki/Information_hiding){:target="_blank" rel="noopener noreferrer"}.
The idea is to hide as much information as you can, because as soon as you expose it, it becomes part of the contract with the outside world.

Hiding data is vital (without this no backwards compatibility)
Splitting apart data allows us to do more work in parallel

However there are some painpoints:
You miss on stuff where data is no longer together

Kafka can help you with that.

It is a nice central backbone - which can be used as a shared place where data can be interchanged.
Event driven architecture is the simplest way to use kafka

Permancence properties: new consumers can pick up these older events.
This allows us to shift our thought processes to make events more permanent and resolve some of the issues of splitting out this data.

Isn’t sharing events not like sharing a database?
An event is much smaller in scope, easier to reason about and easier to keep it compatible

But what about joins? And ad-hoc queries?
=> these events could be stored in another database
[Reporting database pattern](https://martinfowler.com/bliki/ReportingDatabase.html){:target="_blank" rel="noopener noreferrer"}.
As this decouples data about how the application uses this data and how other tools view this data.

[Debezium](https://debezium.io/){:target="_blank" rel="noopener noreferrer"}: allows you to stream changes from your databas.
Change data capture process - it views to the commit log.
Picks up data and send it to another system.
Also very useful for hooking in legacy systems.

Push query - subscribe to results of that query and subscribe to events on that stream.
KsqlDB 

It becomes more interesting if you start crossing the streams (ghostbusteres screenshot)
Push query - cross stream.

Some book suggestions
Good read Ben: [Designing Event-Driven Systems](https://www.confluent.io/designing-event-driven-systems/){:target="_blank" rel="noopener noreferrer"}: good read - and pretty short (add book)

Book: [Monolith to microservices](https://samnewman.io/books/monolith-to-microservices/){:target="_blank" rel="noopener noreferrer"}: Which describes how you can break up your data, among many other things.


### 
(Music and tune switch)

(after keynote - comes the sessions)


(other sessions have already started … 


(lunch sessions - existed - limited to 25 people - idea was to have people have lunch discussion)


## Future of Kafka by Gwen Shapira https://twitter.com/gwenshap
This was a more dynamic session, not recorded, also with a limited amount of people who could join.

There was a chat window - but when trying to read that you lost focus of the screen and the speaker, which is kinda annoying.

### The previous day
{:.no_toc}
It started with some basic questions, one of these was about what other people found interesting talks of the previous day.
So because I wasn't there and have no idea which might be interesting, I have also recorded those answers
* kafka summit activision going from batch to streaming (was an interesting one about going from batch to streaming)
* Talk about bloomberg about a stretch cluster
* I liked KafkaConsumer - Decoupling Consumption and Processing for Better Resource Utilization, had some interesting points
* I recommend Kafka Lag Monitoring For Human Beings

### Stretching your sanity
{:.no_toc}
Working with 2 cloud providers apparently stretches your sanity, the APIs are similar but behaviour is different. 
Like for example disk usage: some providers allow you to expand the disk when the machine is running while one doesn’t.
All those things make it so that a lot of your automation goes out of the window, or that it becomes a forest of if statements.

### Q & A
{:.no_toc}
#### I was intrigued by your wish list for the future of Kafka. I was hoping to hear some of those ideas you were afraid to say during the keynote.
{:.no_toc}

Debugability and observabilty are key requirements by Gwen for her team (kafka as a service on cloud providers

Concerns about the network layer - lot of stuff was made when Netty was still new (discovery burden is part of the protocol) does not play well with Kubernetes

And other efficiency and scalability improvements.

#### As GDPR is a big topic in Europe, any approaches efforts to support crypto shredding natively?
{:.no_toc}

Currently no encryption at rest and other stuff - There is lots of room to make encryption a 1st lvl concern for Kafka

#### You just mentioned a new edition of the definitive guide, what will be the new additions? Also thx for it, it helps a lot to understand the internals.
{:.no_toc}

Big changes around administration, new metrics to monitor and new best practices.
And a chapter around security.

Early publish for next week

#### I heard a lot about if exactly once semantics but never saw a clear example or in house solution for consumer side. Any improvement on Consumer API related to that. Thanks , you're an amazing speaker!
{:.no_toc}

On the consumer side: configuration - either get committed messages as a consumer or you get all.
If you only get committed messages it will increase the latency.
Producer should write the offsets (if you are a consumer - producer)

#### Hi My key wishlist of Kafka is that a put on a Kafka topic can participate in an XA two phase commit
{:.no_toc}

This type of integration should be part of the connect framework


#### Are there any tools that you know of to help manage/document/understand the web of topics, producers, consumers, and streams? I'm thinking about Operations staff effectively managing and multi-tenant environment, or Architects discovering and domain ownership and modeling new change.
{:.no_toc}

Strimzi
Can integrate nicely with github - yams describing topics

#### Any intended future support for postponed messages (scheduled messages) ?
{:.no_toc}

No - main challenge is if this can be implemented performantly

#### I'm curious whether there are anything planned to support more topic/partitions in a Kafka cluster (besides KIP-500). In particular I found that the Java producer client has some bottlenecks when publishing to a wide range of topics, (due to usage of CopyOnWriteMap and GC pressure). Also wondering if there folks out there trying push partition limit boundaries and can share some tips :)
{:.no_toc}

Fantastic question
Lots of time was spent on improving performance of the brokers / producers

Partition limit -> with big clusters there can be a lot of replication overhead.

#### How efficient of the native compression of Kafka? Should we compress the content by ourself?
{:.no_toc}

No, don’t compress data yourselves.
Kafka can send messages via batches (delay configuration)
Kafka compresses the batches - since these batches tend to contain lots of related data so this gets better compression result
There are 4 algorithms to choose from

#### We are having issues with configuring load balancer in front of the RestProxies for consumers with certificate based authentication. anything new comming up in improving in this area? thank you.
{:.no_toc}

There is currently lots of tooling improvements in progress


## Learnings from the Field. Lessons from Working with Dozens of Small & Large Deployments

https://www.confluent.io/resources/kafka-summit-2020/learnings-from-the-field-lessons-from-working-with-dozens-of-small-large-deployments/

In this session lots of hints and tips of using Kafka were addressed.

Make sure to upgrade often, as new releases always contain lots of bug fixes and other small improvements.

Another very important topic to consider are quotas.

One of the most important ones is the leader replication throttle rate 'leader.replication.throttled.rate'.
Because by default a rebalance is unthrottled, which means that it can use up all the bandwith of your cluster.

Also there are client quotas to consider: bandwidth and client quotas.

An interesting session, be sure to check it out if you are planning on running kafka, or make sure that you do not fall in the same pittfalls as countless others have done for you ;-)
## Exactly-Once Made Easy: Transactional Messaging Improvement for Usability & Scalability

By Boyang Chen and [Guozhang Wang](https://twitter.com/guozhangwang)

https://www.confluent.io/resources/kafka-summit-2020/exactly-once-made-easy-transactional-messaging-improvement-for-usability-and-scalability

If you want to learn more about how exactly-once was implemented in kafka you should watch this talk.
It describes how kafka transactions work and how these came to be.

An important note is that it will only work if your app is only consuming and producing to kafka.
Transactions with other (external) systems, like a database, can not be covered by this solution.

Only 1 transaction per partition is possible, this will also have an impact on performance.

## Experienced Developers
This session was one of those discussion ones and it was full.

I am not sure what the limit is of these rooms, but that is kinda annoying.
It would be nice if they could show a counter on the schedule about how full that session is, it is all virtual so that should be possible


## Maximize Business Value of Machine Learning and Data Science with Kafka

https://www.confluent.io/resources/kafka-summit-2020/maximize-the-business-value-of-machine-learning-and-data-science-with-kafka
By Chirag Dadia & Tom Szumowski, Nuuly

Nuuly is the company which rents out clothing via a monhtly subscription.

Kafka streams is used pretty heavily, all micro-services have stream processors, these processors tend to handle most of the business logic.

Kafka is used as the persistent data store, this actually allows them to use their Kafka subsystem and services as a raw data lake which feeds into the downstream applications.
Most workflows are asychronously processed.

KSQL is currently being investigated as it is now a managed service, but not yet actively used.

Kafka is the backbone at Nuuly, it holds all the data, it is the business logic processor and it is orchestrating all the workflows.
Adding a new step into a workflow is as simple as adding a new processor / microservice and the needed topics, easy.

Some use cases where Kafka is used within Nuuly:
* Personalization: recommendation.
* Order fullfillment: minimize the walking path in the warehouse.
* Online optimization: real-time feedback streams for user feedback.

For each of these use cases they go deeper into how these were implemented in the talk.

## Some other stuff I noticed
(No pauses between sessions - not nice ...)
(The sessions last 1 hour, but sessions start every 30 minutes - 2 blocks to sessions … can be annoying when you want to follow overlapping sessions, but does give you more options to switch when there are no / less interesting sessions in 1 block)
(There was a link to speak with a tech to address tech issues)

Badges for attentind sponsor booths
But they were quite sparse - somewhat understandable because it is all pretty short notice.
But I didn't really feel like chatting directly with one of the persons, because I was just looking around, but this is something which might be an interesting avenu to explore.
Have companies be able to use their own virtual booth, which they can 'bring along' to every virtual conference, but that is going to be pretty hard to setup.

There were not that much discussions among participants.
I admit that in real life a lot of the attendees do not join in these discussions and they just move from talk to talk but for me that is an important part of a conference.
Often those side discussion provide you with much more insight and knowledge then any of the talks on schedule.

# Final Thoughts


 
