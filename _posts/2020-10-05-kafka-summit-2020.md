---
layout: post
authors: [tom_van_den_bulck]
title: 'Kafka Summit 2020'
image: /img/2020-09-25-ghelamco-alert/rpi-front.jpg
tags: [Kafka, Conference]
category: Cloud
comments: true
---

# Table of contents
{:.no_toc}

- TOC
{:toc}

----

# Introduction

Attending conferences in these times tends to be a pretty new experience, with everything being virtual.

The Kafka Summit 2020 in London was cancelled, but the one in Austin was not.
However, it was transformed.
Transformed in a new format, completely COVID proof, fully remote and virtual.

I love conferences, as these bring me knew knowledge and insights.

But, my experience with virtual ones is quite limited.

Please note, working remote is no issue, as I have become a true senior remote developer in the last 6 months.


# Day 2

## Where is Day 1?
Timezone difference combined with a busy day at work is not that ideal.

<<understatement>>
    
Since the coffee was no longer helping I decided to dive in the bed in order to charge up the batteries for the next day.

## Keynote The Tyranny of Data by Sam Newman

https://www.confluent.io/resources/kafka-summit-2020/sam-newman-building-microservices-keynote-the-tyranny-of-data


Data is the lifeblood of what we do, 75% of your time is taking data out of a database and putting it on a screen.

A relational DB is a wonderful thing, it has transactions, it is robust.
But it wants to have all the data and they tend to become too big to fail.
At which point it becomes that important that we are afraid to change it.


Breaking apart data can be interesting to reduce the scope of fulfilling regulations.


Property of independent deployability: that is what we want from a microservice architecture.
Distributing responsibility avoid the need to constantly coördinate between teams.

It is important to get the boundaries of the services right - together with independent deployability it can be possible.

It is important to maintain backwards compatibility.
There is an idea from 1971: [Information hiding](https://en.wikipedia.org/wiki/Information_hiding){:target="_blank" rel="noopener noreferrer"}.
The idea is to hide as much information as you can, because as soon as you expose it, it becomes part of the contract with the outside world.

Hiding data is vital (without this no backwards compatibility)
Splitting apart data allows us to do more work in parallel

However there are some painpoints:
You miss on stuff where data is no longer together


Kafka can help you with that.

It is a nice central backbone - which can be used as a shared place where data can be interchanged.
Event driven architecture is the simplest way to use kafka

Permancence properties: new consumers can pick up these older events.
This allows us to shift our thought processes to make events more permanent and resolve some of the issues of splitting out this data.

Isn’t sharing events not like sharing a database?
An event is much smaller in scope, easier to reason about and easier to keep it compatible

But what about joins? And ad-hoc queries?
=> these events could be stored in another database
[Reporting database pattern](https://martinfowler.com/bliki/ReportingDatabase.html){:target="_blank" rel="noopener noreferrer"}.
As this decouples data about how the application uses this data and how other tools view this data.

[Debezium](https://debezium.io/){:target="_blank" rel="noopener noreferrer"}: allows you to stream changes from your databas.
Change data capture process - it views to the commit log.
Picks up data and send it to another system.
Also very useful for hooking in legacy systems.

Push query - subscribe to results of that query and subscribe to events on that stream.
KsqlDB 

It becomes more interesting if you start crossing the streams (ghostbusteres screenshot)
Push query - cross stream.


Some book suggestions
Good read Ben: [Designing Event-Driven Systems](https://www.confluent.io/designing-event-driven-systems/){:target="_blank" rel="noopener noreferrer"}: good read - and pretty short (add book)

Book: [Monolith to microservices](https://samnewman.io/books/monolith-to-microservices/){:target="_blank" rel="noopener noreferrer"}: Which describes how you can break up your data, among many other things.




### 
(Music and tune switch)

(after keynote - comes the sessions)


(other sessions have already started … 


(lunch sessions - existed - limited to 25 people - idea was to have people have lunch discussion)




## Future of Kafka by Gwen Shapira https://twitter.com/gwenshap
This was a more dynamic session, not recorded, also with a limited amount of people who could join.

There was a chat window - but when trying to read that you lost focus of the screen and the speaker, which is kinda annoying.

### The previous day
{:.no_toc}
It started with some basic questions, one of these was about what other people found interesting talks of the previous day.
So because I wasn't there and have no idea which might be interesting, I have also recorded those answers
* kafka summit activision going from batch to streaming (was an interesting one about going from batch to streaming)
* Talk about bloomberg about a stretch cluster
* I liked KafkaConsumer - Decoupling Consumption and Processing for Better Resource Utilization, had some interesting points
* I recommend Kafka Lag Monitoring For Human Beings

### Stretching your sanity
{:.no_toc}
Working with 2 cloud providers apparently stretches your sanity, the APIs are similar but behaviour is different. 
Like for example disk usage: some providers allow you to expand the disk when the machine is running while one doesn’t.
All those things make it so that a lot of your automation goes out of the window, or that it becomes a forest of if statements.

### Q & A
{:.no_toc}
#### I was intrigued by your wish list for the future of Kafka. I was hoping to hear some of those ideas you were afraid to say during the keynote.
{:.no_toc}

Debugability and observabilty are key requirements by Gwen for her team (kafka as a service on cloud providers

Concerns about the network layer - lot of stuff was made when Netty was still new (discovery burden is part of the protocol) does not play well with Kubernetes

And other efficiency and scalability improvements.

#### As GDPR is a big topic in Europe, any approaches efforts to support crypto shredding natively?
{:.no_toc}

Currently no encryption at rest and other stuff - There is lots of room to make encryption a 1st lvl concern for Kafka

#### You just mentioned a new edition of the definitive guide, what will be the new additions? Also thx for it, it helps a lot to understand the internals.
{:.no_toc}

Big changes around administration, new metrics to monitor and new best practices.
And a chapter around security.

Early publish for next week

#### I heard a lot about if exactly once semantics but never saw a clear example or in house solution for consumer side. Any improvement on Consumer API related to that. Thanks , you're an amazing speaker!
{:.no_toc}

On the consumer side: configuration - either get committed messages as a consumer or you get all.
If you only get committed messages it will increase the latency.
Producer should write the offsets (if you are a consumer - producer)

#### Hi My key wishlist of Kafka is that a put on a Kafka topic can participate in an XA two phase commit
{:.no_toc}

This type of integration should be part of the connect framework


#### Are there any tools that you know of to help manage/document/understand the web of topics, producers, consumers, and streams? I'm thinking about Operations staff effectively managing and multi-tenant environment, or Architects discovering and domain ownership and modeling new change.
{:.no_toc}

Strimzi
Can integrate nicely with github - yams describing topics

#### Any intended future support for postponed messages (scheduled messages) ?
{:.no_toc}

No - main challenge is if this can be implemented performantly

#### I'm curious whether there are anything planned to support more topic/partitions in a Kafka cluster (besides KIP-500). In particular I found that the Java producer client has some bottlenecks when publishing to a wide range of topics, (due to usage of CopyOnWriteMap and GC pressure). Also wondering if there folks out there trying push partition limit boundaries and can share some tips :)
{:.no_toc}

Fantastic question
Lots of time was spent on improving performance of the brokers / producers

Partition limit -> with big clusters there can be a lot of replication overhead.

#### How efficient of the native compression of Kafka? Should we compress the content by ourself?
{:.no_toc}

No, don’t compress data yourselves.
Kafka can send messages via batches (delay configuration)
Kafka compresses the batches - since these batches tend to contain lots of related data so this gets better compression result
There are 4 algorithms to choose from

#### We are having issues with configuring load balancer in front of the RestProxies for consumers with certificate based authentication. anything new comming up in improving in this area? thank you.
{:.no_toc}

There is currently lots of tooling improvements in progress


## Learnings from the Field. Lessons from Working with Dozens of Small & Large Deployments

https://www.confluent.io/resources/kafka-summit-2020/learnings-from-the-field-lessons-from-working-with-dozens-of-small-large-deployments/


Rebalancelistener or application health state listeners so you can share when stuff is rebalancing.

Kubernetes - Control Center
There is an API on the broker you can use for admin functions (for broker creation and such)

Deployment on kubernets is easy - keeping it running is hard
Helm Charts is not production ready for that



## Exactly-Once Made Easy: Transactional Messaging Improvement for Usability & Scalability

https://www.confluent.io/resources/kafka-summit-2020/exactly-once-made-easy-transactional-messaging-improvement-for-usability-and-scalability

Note: it will work if your app is only consuming and producing to kafka

Only 1 transaction per partition


## Experienced Developers
This session was one of those discussion ones and it was full.

I am not sure what the limit is of these rooms, but that is kinda annoying.
It would be nice if they could show a counter on the schedule about how full that session is, it is all virtual so that should be possible

## Devops on Kafka

https://www.confluent.io/resources/kafka-summit-2020/organic-growth-and-a-good-night-sleep-effective-kafka-operations-at-pinterest

Rolling upgrades are the way to go.
Not just delete the pod.
But delete with extra logic: first make sure nothing is underreplicated before deleting the pod

Helm Chart will get you from 0 to 60 - but you will still have to manage it.
You will still need an operator to keep it running pretty stable.


Nuuly is the company (cloths rental)

Databases with the stream processors have a local kstore

KSQL applications are currently being investigated as it is now a managed service

Data is stored in kafka, all data is stored in kafka, all services have stream processors.
Kafka orchestrates the workflows.

Adding a new step is adding a new processor / microservice and the needed topics

Order fullfillment: minimize walking path in warehouse
Route planning algorithms tend to be very cpu intensive
Routes need to be send out rapidly




## Maximize Business Value of Machine Learning and Data Science with Kafka

https://www.confluent.io/resources/kafka-summit-2020/maximize-the-business-value-of-machine-learning-and-data-science-with-kafka

## Some other stuff I noticed
(No pauses between sessions - not nice ...)
(The sessions last 1 hour, but sessions start every 30 minutes - 2 blocks to sessions … can be annoying when you want to follow overlapping sessions, but does give you more options to switch when there are no / less interesting sessions in 1 block)
(There was a link to speak with a tech to address tech issues)

Badges for attentind sponsor booths
But they were quite sparse - somewhat understandable because it is all pretty short notice.
But I didn't really feel like chatting directly with one of the persons, because I was just looking around, but this is something which might be an interesting avenu to explore.
Have companies be able to use their own virtual booth, which they can 'bring along' to every virtual conference, but that is going to be pretty hard to setup.

There were not that much discussions 

# Final Thoughts


 
